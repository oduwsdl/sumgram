# sumgram

sumgram is a tool that summarizes a collection of text documents by generating the most frequent sumgrams (multiple ngrams) in the collection. Unlike convention ngram generators that split multi-word proper nouns, sumgram works hard to avoid this by applying two ([`pos_glue_split_ngrams`](#pos_glue_split_ngrams) and [`mvg_window_glue_split_ngrams`](#mvg_window_glue_split_ngrams)) algorithms. The algorithms also enable sumgram to generate multiple ngrams, or "sumgrams" (bigrams, trigrams, k-grams, etc.) as part of the summary, instead of limiting the summary to a single ngram class (e.g., bigrams).

From Fig. 1, the 4-gram "centers for disease control and prevention" was split into 3 different  bigrams ("centers disease", "disease control", and "control prevention") by a conventional algorithm that generates bigrams. But sumgram detected and "glued" such split ngrams.

*Fig. 1: Comparison of top 20 bigrams and top 20 sumgrams (multiple ngrams) generated by sumgram for a collection of documents about the [2014 Ebola Virus Outreak](https://en.wikipedia.org/wiki/Western_African_Ebola_virus_epidemic). Proper nouns of more than two words (e.g., "centers for disease control and prevention") are split when generating bigrams, sumgram strives to remedy this.*
![ebola ngrams](pics/sumgrams_ebola.png)

*Fig. 2: Comparison of top 20 bigrams and top 20 sumgrams (multiple ngrams) generated by sumgram for a collection of documents about [Hurricane Harvey](https://en.wikipedia.org/wiki/Hurricane_Harvey). Proper nouns of more than two words (e.g., "federal emergency management agency") are split when generating bigrams, sumgram strives to remedy this.*
![hurricane harvey ngrams](pics/sumgrams_harvey.png)
## Additional Features
In addition to generating top sumgrams, sumgram ranks sentences and documents.
### Ranking documents (`--no-rank-sentences` to switch off)
`get_ranked_docs()` ranks documents by giving credit to documents that have highly ranked terms in the ranked list of ngrams. A document's score is awarded by accumulating the points awarded by the position of terms in the ranked list of ngrams. Please note that documents without terms in ranked list of ngrams are not awarded points. Therefore, some documents may not be ranked because they performed poorly - did not have any term in the ranked list of ngrams.

### Ranking sentences (`--no-rank-docs` to switch off)
`rank_sents_frm_top_ranked_docs()` ranks sentences in the top ranked documents exclusively, and gives credit to sentences with a high average overlap between the sentence tokens and the tokens in the top ngrams. For all sentences in a top ranked documents, a sentence's score (average overlap) is measured by calculating the average overlap between the terms in the top ngrams and the given sentence. This accounts for how many different tokens in the top ngrams that are present in a sentence.

## Installation
Just type
```
$ pip install sumgram
```
OR
```
$ git clone https://github.com/oduwsdl/sumgram.git
$ cd sumgram; pip install .; cd ..; rm -rf sumgram;
```
## Recommended Requirement and Performance Considerations
### Recommended Requirement
For the best results, we recommend [installing and running Stanford CoreNLP Server](https://ws-dl.blogspot.com/2018/03/2018-03-04-installing-stanford-corenlp.html) for two reasons.
First, the "pos" in `pos_glue_split_ngrams` stands for Parts Of Speech. This algorithm needs a Part of Speech annotator in order to "glue" split ngrams, hence the need for Stanford CoreNLP server. However, if you do not install Stanford CoreNLP Server, sumgram is robust enough to attempt to glue split ngrams with the second algorithm `mvg_window_glue_split_ngrams`. 

Second, as part of ranking sentences, sumgram needs to segment the sentences in the documents. Stanford CoreNLP's [`ssplit`](https://stanfordnlp.github.io/CoreNLP/ssplit.html) annotator splits sentences after tokenization, and exploits the decisions of the tokenizer. Probabilitic methods (such as `ssplit`) for segmenting sentences often outperform rule-based methods that use regular expressions to define sentence boundaries. If you do not install Stanford CoreNLP however, sumgram will adopt a regular expression (`[.?!][ \n]|\n+`) to mark sentence boundaries. This rule can be passed (```--sentence-pattern``` - command line, ```sentence_pattern``` - python) as an argument to sumgram.

### Performance Considerations
`pos_glue_split_ngrams` imposes additional runtime overhead on sumgram. You may choose to force sumgram to avoid using the ssplit annotator (implicitly switching off `pos_glue_split_ngrams`) by setting `--sentence-tokenizer=regex`. Please note that the command line argument `--no-pos-glue-split-ngrams` does not switch off Stanford CoreNLP's ssplit, it merely avoids the use of the `pos_glue_split_ngrams`.

By default `--no-parent-sentences` is switched off, this means that the sentences that mention the top sumgrams are included in the final dictionary output of sumgram (output of `get_top_ngrams()`), thus increasing the size of the output. To avoid this, include the `--no-parent-sentences` option.

## Usage
### Basic usage:
```
$ sumgram path/to/collection/of/text/files/
```
### Python script usage:
[Command line options](#full-usage) may be activated by setting the argument in the `params` dictionary passed as an argument to `get_top_ngrams()`. To set a single or multi-character command line argument, consider the following transformation example:

```
params = {}
params['n'] = 2                      #For command line argument -n
params['output'] = 'sumgrams.json'   #For command line argument --output
params['sentences_rank_count'] = 20  #For command line argument --sentences-rank-count
```

The following is an example Python script usage of sumgram done by calling the `get_top_ngrams()` function.
```
import json
from sumgram.sumgram import get_top_ngrams

doc_lst = [
    {'id': 0, 'text': 'The eye of Category 4 Hurricane Harvey is now over Aransas Bay. A station at Aransas Pass run by the Texas Coastal Observing Network recently reported a sustained wind of 102 mph with a gust to 132 mph. A station at Aransas Wildlife Refuge run by the Texas Coastal Observing Network recently reported a sustained wind of 75 mph with a gust to 99 mph. A station at Rockport reported a pressure of 945 mb on the western side of the eye.'},
    {'id': 1, 'text': 'Eye of Category 4 Hurricane Harvey is almost onshore. A station at Aransas Pass run by the Texas Coastal Observing Network recently reported a sustained wind of 102 mph with a gust to 120 mph.'},
    {'id': 2, 'text': 'Hurricane Harvey has become a Category 4 storm with maximum sustained winds of 130 mph. Sustained hurricane-force winds are spreading onto the middle Texas coast.'}
  ]
params = {
    'top_ngram_count': 10,
    'add_stopwords': 'image',
    'no_rank_sentences': True,
    'title': 'Top sumgrams for Hurricane Harvey text collection'
}

ngram = 2
sumgrams = get_top_ngrams(doc_lst, ngram, params=params)
with open('sumgrams.json', 'w') as outfile:
  json.dump(sumgrams, outfile)
```
### Examples:
### Generate top 10 (t = 10) sumgrams for the [Archive-It Ebola Virus Collection](https://archive-it.org/collections/4887):
```
$ sumgram -t 10 cols/ebola/
 rank  ngram                                                TF   TF-Rate
  1    in west africa                                       50    0.35 
  2    liberia and sierra leone                             46    0.33 
  3    ebola virus                                          44    0.31 
  4    ebola outbreak                                       41    0.29 
  5    public health                                        40    0.28 
  6    the centers for disease control and prevention       23    0.16 
  7    the united states                                    23    0.16 
  8    the world health organization                        22    0.16 
  9    ebola patients                                       20    0.14 
  10   health workers                                       20    0.14 
``` 
### Generate top 10 (t = 10) sumgrams for the [Archive-It Hurricane Harvey Collection](https://archive-it.org/collections/9323):
```
$ sumgram -t 10 cols/harvey/
rank  ngram                                                TF   TF-Rate
  1    hurricane harvey                                     20    0.47 
  2    tropical storm harvey                                10    0.23 
  3    2017 houston transtar inc.                           9     0.21 
  4    2017. photo                                          9     0.21 
  5    corpus christi                                       9     0.21 
  6    image 28 of                                          9     0.21 
  7    image 29 of                                          9     0.21 
  8    image 30 of                                          9     0.21 
  9    image 31 of                                          9     0.21 
  10   image 32 of                                          9     0.21
``` 
This collection has lots of images, but the "image" term might obscure more salient ngrams, so let's 
rerun the command, but this time consider "image" a stopword. As seen below such modification exposed more salient bigrams such as "buffalo bayou" and "coast guard"

```
$ sumgram -t 10 --add-stopwords="image" cols/harvey/
 rank  ngram                                                TF   TF-Rate
  1    hurricane harvey                                     20    0.47 
  2    tropical storm harvey                                10    0.23 
  3    2017 houston transtar inc.                           9     0.21 
  4    2017. photo                                          9     0.21 
  5    corpus christi                                       9     0.21 
  6    texas photo                                          9     0.21 
  7    27, 2017                                             8     0.19 
  8    buffalo bayou                                        8     0.19 
  9    coast guard                                          8     0.19 
  10   harvey photo                                         8     0.19 
``` 

### Use application from Python Docker container to generate top 10 (t = 10) sumgrams for the [Archive-It Hurricane Harvey Collection](https://archive-it.org/collections/9323):
```
$ docker run -it --rm --name my-running-script -v "$PWD":/usr/src/myapp -w /usr/src/myapp --network=host python:3.7.3-stretch bash
$ sumgram -t 10 --add-stopwords="image" cols/harvey/
 rank  ngram                                                TF   TF-Rate
  1    hurricane harvey                                     20    0.47 
  2    tropical storm harvey                                10    0.23 
  3    2017 houston transtar inc.                           9     0.21 
  4    2017. photo                                          9     0.21 
  5    corpus christi                                       9     0.21 
  6    texas photo                                          9     0.21 
  7    27, 2017                                             8     0.19 
  8    buffalo bayou                                        8     0.19 
  9    coast guard                                          8     0.19 
  10   harvey photo                                         8     0.19 
``` 

### Generate top sumgrams from collection of URLs:
This requires dereferencing the URLs and [removing the HTML boilerplate](https://ws-dl.blogspot.com/2017/03/2017-03-20-survey-of-5-boilerplate.html). This example requires the [installation of NwalaTextUtils](https://github.com/oduwsdl/NwalaTextUtils).
```
import json
from NwalaTextUtils.textutils import prlGetTxtFrmURIs
from sumgram.sumgram import get_top_ngrams

ngram = 2
uris_lst = [
  'http://www.euro.who.int/en/health-topics/emergencies/pages/news/news/2015/03/united-kingdom-is-declared-free-of-ebola-virus-disease',
  'https://time.com/3505982/ebola-new-cases-world-health-organization/',
  'https://www.scientificamerican.com/article/why-ebola-survivors-struggle-with-new-symptoms/'
]

doc_lst = prlGetTxtFrmURIs(uris_lst)
sumgrams = get_top_ngrams(doc_lst, ngram)

with open('sumgrams.json', 'w') as outfile:
    json.dump(sumgrams, outfile)
```

### Full usage
```
sumgram [options] path/to/collection/of/text/files/

Options:
-n=2                                      Base n (int) for generating top ngrams, if n = 2, bigrams are generated
-o, --output                              Output file
-s, --sentences-rank-count=10             The count of top ranked sentences to generate
-t, --top-ngram-count=10                  The count of top ngrams to generate

--add-stopwords                           Comma-separated list of addition stopwords
--corenlp-host=localhost                  Stanford CoreNLP Server host (needed for decent sentence tokenizer)
--corenlp-port=9000                       Stanford CoreNLP Server port (needed for decent sentence tokenizer)
--corenlp-max-sentence-words=100          Stanford CoreNLP maximum words per sentence
--include-postings=False                  Include inverted index of term document mappings

--log-file                                Log output filename
--log-format                              Log print format, see: https://docs.python.org/3/howto/logging-cookbook.html
--log-level=info                          Log level from OPTIONS: {critical, error, warning, info, debug, notset}

--mvg-window-min-proper-noun-rate=0.5     Mininum rate threshold (larger, stricter) to consider a multi-word proper noun a candidate to replace an ngram
--ngram-printing-mw=50                    Mininum width for printing ngrams

--no-mvg-window-glue-split-ngrams=False   Do not glue split top ngrams with Moving Window method (default is False)
--no-parent-sentences                     Do not include sentences that mention top ngrams in top ngrams payload (default is False)
--no-pos-glue-split-ngrams=False          Do not glue split top ngrams with POS method (default is True)
--no-rank-docs=False                      Do not rank documents flag
--no-rank-sentences=False                 Do not rank sentences flag

--pos-glue-split-ngrams-coeff=2           Coeff. for permitting matched ngram replacement. Interpreted as 1/coeff, e.g., 1/2
--pretty-print=False                      Pretty print JSON output
--rm-subset-top-ngrams-coeff=2            Coeff. for permitting matched ngram replacement. Interpreted as 1/coeff, e.g., 1/2

--sentence-pattern='[.?!][ \n]|\n+'       For sentence ranking: Regex string that specifies tokens for sentence tokenization
--sentence-tokenizer=ssplit               For sentence ranking: Method for segmenting sentences, options: {ssplit, regex}
--shift=0                                 Factor to shift top ngram calculation
--token-pattern                           Regex string that specifies tokens for document tokenization. Default = '\b[a-zA-Z\'\â€™-]+[a-zA-Z]+\b|\d+[.,]?\d*'
--title                                   Text label to be used as a heading when printing top ngrams
```

### Algorithms for detecting and gluing split Multi-Word Proper Noun (MWPN) ngrams

## pos_glue_split_ngrams

This algorithm is the first measure to merge split MWPN ngrams. For example, the ngram fragment 
`emergency management` 
was split (base ngram = 2) from its parent MWPN:
`federal emergency management agency`. `pos_glue_split_ngrams` attempts to replace the ngram fragment with its MWPN.

The `pos_glue_split_ngrams` process is outlined as follows:
* All tokens in all the sentences are labeled with their respective Parts of Speech (POS) with Stanford CoreNLP's POS annotator.
* WMPNs are identified by this rule: a WMPN is a `contiguous sequence of NNP` or `a contiguous sequence of NNP interleaved with CC or IN`. For example, given the follow [POS label descriptions](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)
  ```
  NNP: Proper Noun Singular
  NNPS: Proper Noun plural
  CC: Coordinating conjunction
  IN: Preposition or subordinating conjunction
  ```
  According to `pos_glue_split_ngrams`, the following ngram sequences are MWPNs:

  ```
  "Hurricane harvey"                    - (NNP NNP)
  "Centers for Disease Control"         - (NNP IN NNP NNP)
  "Federal Emergency Management Agency" - (NNP NNP NNP NNP)
  ```

* Let TF_C = Term Frequency of fragment child ngram (e.g., `emergency management`). Let TF_P = Term Frequency of MWPN (e.g., `federal emergency management agency`). `pos_glue_split_ngrams` replaces a fragment child ngram with a parent MWPN, if `TF_P > TF_C / pos_glue_split_ngrams_coeff`. The restriction is done in order to avoid replacing a high-quality fragment child ngram (e.g., `tropical storm` with TF_C: 121) with a poor-quality MWPN (e.g., `ddhhmm tropical storm harvey discussion number` with TF_P: 5)

## mvg_window_glue_split_ngrams

This algorithm is the second measure to merge split MWPN ngrams. Unlike `pos_glue_split_ngrams` which relies on a rule for identifying MWPNs, `mvg_window_glue_split_ngrams` seeks out terms (controlled by an expanding window size) that are frequently left and right neighbors of the fragment ngrams. 

For example, from the Hurricane Harvey collection, the ngram fragment `emergency management`, often had the term `federal` to its left, and `agency` to its right. The concatenation of the fragment ngram and its highly frequent left and right neighbors is considered a MWPN: `federal` + `emergency management` + `agency`. If the co-occurrence rate of the MWPN >= `mvg_window_min_proper_noun_rate`, `mvg_window_glue_split_ngrams` replaces the child fragment ngram (`emergency management`) with its parent MWPN (`federal emergency management agency`).

Snippet of `mvg_window_glue_split_ngrams` process:
```
mvg_window_min_proper_noun_rate: 0.5 (Let the MWPN occur at least 50% of the time)
window_size: 1
fragment ngram: ['emergency', 'management']
candidate sentence tokens: 

['more', 'than', '32', '000', 'people', 'have', 'been', 'housed', 'in', 'shelters', '', 
'and', 'the', 'federal', 'emergency', 'management', 'agency', 'is', 'expecting', 'nearly', 
'a', 'half', 'million', 'people', 'to', 'seek', 'some', 'sort', 'of', 'disaster', 'aid.']

#Here are the candidate MWPNs generated by adding 1 (window_size = 1) term to the left, right, and both left and right of the fragment ngram:

left  MWPN: "federal emergency management"        occurrence rate: 0.875
right MWPN: "emergency management agency"         occurrence rate: 0.625
both  MWPN: "federal emergency management agency" occurrence rate: 0.625 
```

`mvg_window_glue_split_ngrams` favors longer MWPNs as long as its occurrence rate >= `mvg_window_min_proper_noun_rate`, so even though the left MWPN's (`federal emergency management`) occurrence rate is the highest (0.875), the algorithm would select the longest MWPN (`federal emergency management agency`) since it fulfills the selection criteria: its occurrence rate (0.625 > mvg_window_min_proper_noun_rate = 0.625 > 0.5). For each fragment ngram, `mvg_window_glue_split_ngrams` searches for the longest MWPN that fulfills the selection criteria.
